{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=r'E:/Edvancer/Python/2_Linear Regression/QU/loan_data_train.csv'\n",
    "test_file=r'E:/Edvancer/Python/2_Linear Regression/QU/loan_data_test.csv'\n",
    "\n",
    "ld_train=pd.read_csv(train_file)\n",
    "ld_test=pd.read_csv(test_file)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets combine the data for data prep\n",
    "\n",
    "ld_test['Interest.Rate']=np.nan\n",
    "ld_train['data']='train'\n",
    "ld_test['data']='test'\n",
    "\n",
    "#When we add intrest rate as NA, it should be in same as order of train data . I.e, interset rate should come as 4th column only\n",
    "#hence we use ld_test[ld_train.columns] which ensure that test column order are same as train columns\n",
    "\n",
    "ld_test=ld_test[ld_train.columns]\n",
    "ld_all=pd.concat([ld_train,ld_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID,Amount.Funded.By.Investors : drop \n",
    "# Interest Rate , Debt to income ratio : remove % and then to numeric\n",
    "# Amount.Requested , 'Open.CREDIT.Lines','Revolving.CREDIT.Balance': convert it to numeric \n",
    "# FICO.Range : replace it by a numeric column which is average of the range\n",
    "# Employment Length : convert to number\n",
    "# Loan Lenth, Loan Purpose , State , Home ownership: dummies for categories with good occurence rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#axis=1 : Means columns(look for labels and column axis and it will drop that)\n",
    "#inplace=true (make inplace changes(update there only), Dont want to assign data back to data itself )\n",
    "\n",
    "ld_all.drop(['ID','Amount.Funded.By.Investors'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % will get removed,but it will not convert into char till yet\n",
    "\n",
    "for col in ['Interest.Rate','Debt.To.Income.Ratio']:\n",
    "    ld_all[col]=ld_all[col].str.replace(\"%\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many columns which should have really been numbers have been imported as character columns , probably because some characters values in those columns in the files such as % or dot etc . We'll convert all such columns to numbers ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.to_numeric :- Convert into numeric\n",
    "#errors=\"coerce\":- whenever it is not poss to  convert into numeric,we/it will set NA \n",
    "\n",
    "for col in ['Amount.Requested', 'Interest.Rate','Debt.To.Income.Ratio',\n",
    "            'Open.CREDIT.Lines','Revolving.CREDIT.Balance']:\n",
    "    ld_all[col]=pd.to_numeric(ld_all[col],errors='coerce')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_all['FICO.Range'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#astype(float): average might be decimal thats why\n",
    "#first we split\n",
    "#then took avg\n",
    "#k[0]+k[1]/2 :- devide by 2 means 1/2 i.e 0.5\n",
    "\n",
    "k=ld_all['FICO.Range'].str.split(\"-\",expand=True).astype(float)\n",
    "\n",
    "ld_all['fico']=0.5*(k[0]+k[1])\n",
    "\n",
    "del ld_all['FICO.Range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_all['fico'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_all['Employment.Length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str[:2] means first two\n",
    "\n",
    "ld_all['Employment.Length']=ld_all['Employment.Length'].str.replace('years',\"\")\n",
    "\n",
    "ld_all['Employment.Length']=ld_all['Employment.Length'].str.replace('year',\"\")\n",
    "\n",
    "ld_all['Employment.Length']=np.where(ld_all['Employment.Length'].str[:2]==\"10\",10,ld_all['Employment.Length'])\n",
    "\n",
    "ld_all['Employment.Length']=np.where(ld_all['Employment.Length'].str[0]==\"<\",0,ld_all['Employment.Length'])\n",
    "\n",
    "ld_all['Employment.Length']=pd.to_numeric(ld_all['Employment.Length'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that to apply string function on pandas data frame columns you need to str attribute\n",
    "\n",
    "cat_cols=ld_all.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we dont want last one i.e data \n",
    "\n",
    "cat_cols=cat_cols[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use below code/loop if you want to ignor categories with low frequencies. In next section logistic regression, we will use panda's get dummies function You can work either of this Ignoring categories with low fre however result in fewer column without affecting model performance too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col in cat_cols:- chheck rowise\n",
    "#freqs=ld[col].value_counts()\n",
    "\n",
    "#K=freqs.index[freqs>20][:-1], here [-1] is except the last.That is it will ignor false one,\n",
    "#and from True one it will create dumnmies except the last thet is n-1 dummies\n",
    "\n",
    "# name=col+'_'+cat:- Appending cat at end of every dummy var name\n",
    "\n",
    "#ld[col]==cat :- It give you logical array as true and false\n",
    "#astype(int):- It will convert true and false to 1 and 0 which gives us a flag variables\n",
    "\n",
    "\n",
    "for col in cat_cols:\n",
    "    freqs=ld_all[col].value_counts()\n",
    "    k=freqs.index[freqs>20][:-1]    #[-1] means except the last i.e it will ignor false one & from True one create n-1 dummies\n",
    "    for cat in k:\n",
    "        name=col+'_'+cat\n",
    "        ld_all[name]=(ld_all[col]==cat).astype(int)\n",
    "    del ld_all[col]\n",
    "    print(col)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ld_all.isnull().sum():- It will not check in intreset rate and data of training data cz there are no missing value in intrset rate\n",
    "#intrest rate= 300 is for test data \n",
    "#whenever there are missing value in col, we are replace it with the mean of same col belonging to training data \n",
    "#why not test?? cz test are we consider as unseen data\n",
    "\n",
    "for col in ld_all.columns:\n",
    "    if (col not in ['Interest.Rate','data'])& (ld_all[col].isnull().sum()>0):\n",
    "        ld_all.loc[ld_all[col].isnull(),col]=ld_all.loc[ld_all['data']=='train',col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data is where the data indicator column is train\n",
    "#Once we are seperated, we dont need the data column any more\n",
    "\n",
    "#From test we always drop response variable and data\n",
    "\n",
    "ld_train=ld_all[ld_all['data']=='train']\n",
    "del ld_train['data']\n",
    "ld_test=ld_all[ld_all['data']=='test']\n",
    "ld_test.drop(['Interest.Rate','data'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove overall combine data\n",
    "del ld_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train1,ld_train2=train_test_split(ld_train,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For any modelling in python, you need to pass predictor and response seperatly\n",
    "#all predictor will goes in xtrain and response will goes in ytrain\n",
    "\n",
    "#1 is axis = 1\n",
    "#we are not using inplace=True cz we are not removing that column from data. \n",
    "#Here it will keep all other variables except the Intrest Rate guy\n",
    "\n",
    "\n",
    "#Build model on ld_train1 using x_train1 and y_train1\n",
    "x_train1=ld_train1.drop('Interest.Rate',axis=1) \n",
    "y_train1=ld_train1['Interest.Rate']             \n",
    "\n",
    "#check performance on ld_train2 using x_train2 and y_train2\n",
    "x_train2=ld_train2.drop('Interest.Rate',axis=1) #Predicted/Observed value Value\n",
    "y_train2=ld_train2['Interest.Rate']  #Actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficient corresponding to each of the values/variables\n",
    "\n",
    "list(zip(x_train1.columns,lm.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance Check\n",
    "\n",
    "predicted_ir=lm.predict(x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(ld_train2['Interest.Rate'],predicted_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "\n",
    "residual=predicted_ir-y_train2\n",
    "rmse_lm=np.sqrt(np.dot(residual,residual)/len(predicted_ir))\n",
    "rmse_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the tentative performance now, lets build the model on entire training to make prediction on test/production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For any modelling in python, you need to pass predictor and response seperatly\n",
    "#all predictor will goes in x_train and response will goes in y_train\n",
    "\n",
    "#ld_train and ld_test are main train test\n",
    "\n",
    "#Buid model using ld_train and check performance on ld_test\n",
    "\n",
    "x_train=ld_train.drop('Interest.Rate',axis=1) #Predictor\n",
    "y_train=ld_train['Interest.Rate'] #Actual/Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=lm.predict(ld_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write these to a csv file for submission like this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_pred).to_csv(\"mysubmission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge  Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that linear regression has produced coefficients for all variables. If you recall our theoretical discussion, we need to penalise coefficient for the variables which are not really contributing well to our resposne and might be causing overfitting of the model. Among the regularised technique we'll first look at Ridge regression.\n",
    "\n",
    "Since penalty in ridge regression is a hyperparameter , we'd look at multiple values of it and choose the best one through 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here lambda is the penalty coefficient and it’s free to take any allowed number while alpha is selected based on the model you want to try\n",
    "#Try lambda value 1 to 100\n",
    "#you can take any\n",
    "\n",
    "lambdas=np.linspace(1,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'alpha':lambdas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When fit_intercept=True, the line of best fit is allowed to \"fit\" the y-axis (close to 100 in this example). \n",
    "#When fit_intercept=False, the intercept is forced to the origin (0, 0).\n",
    "\n",
    "model=Ridge(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearchcv fits the model with best paramter choice on entire train data alredy.\n",
    "\n",
    "Grid search automatically fits the best estimator on the entire data, you can directly use this to make predictions on test_data. But if you want to look at coefficients , its much more convenient to fit the model with direct function\n",
    "\n",
    "GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "if you want to look at coefficient seperatly run ridge model seperatly. check looking at coefficients part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by default GridSearchCV consider higher the better\n",
    "#LM consider lower the better, hence if you replace with negative then higher the better becomes a case\n",
    "\n",
    "grid_search=GridSearchCV(model,param_grid=params,cv=10,scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value of alpha comes 33.0. So we will try between that ranage only\n",
    "No need to expand parameter to try out\n",
    "Try to expand when value comes at edage like near 1 or 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of grid seartch\n",
    "\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " if you want you can now fit a ridge regression model with obtained value of alpha , although there is no need, grid search automatically fits the best estimator on the entire data, you can directly use this to make predictions on test_data. But if you want to look at coefficients , its much more convenient to fit the model with direct function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the report function given below you can see the cv performance of top few models as well, that will the tentative performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(grid_search.cv_results_,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=grid_search.predict(ld_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_pred).to_csv(\"mysubmission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## For looking at coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model=grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(x_train1.columns,ridge_model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that ridge regression though, shrinks the coefficients but never makes them exactly zero, essentially never reduce our model size. Next we look at lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas=np.linspace(1,10,100)\n",
    "model=Lasso(fit_intercept=True)\n",
    "params={'alpha':lambdas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(model,param_grid=params,cv=10,scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that, the best value of alpha comes at the edge of the range that we tried , we should expand the trial range on that side and run this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas=np.linspace(.001,2,100)\n",
    "params={'alpha':lambdas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(model,param_grid=params,cv=10,scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(grid_search.cv_results_,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model=grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(x_train.columns,lasso_model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that lasso regression, not only improves performance on the data slightly , but also makes size of the model smaller by making many coefficents exactly zero, thus excluding them from our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
